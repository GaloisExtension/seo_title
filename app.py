"""
Streamlit + LangChain based keyword & article-volume research tool
------------------------------------------------------------------

ç›®çš„:
- LLMã«ã‚ˆã‚Šå¤§é‡ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆ
- Google / Note / Qiita / Zenn ãªã©ã‹ã‚‰è¨˜äº‹ä»¶æ•°ã‚„ã‚¿ã‚¤ãƒˆãƒ«/ã‚¿ã‚°ã‚’å–å¾—
- LLMã§å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºãƒ»ã‚®ãƒ£ãƒƒãƒ—åˆ†æ

ã“ã®1ãƒ•ã‚¡ã‚¤ãƒ«ç‰ˆã¯ **ã¨ã‚Šã‚ãˆãšå‹•ãæœ€å°æ§‹æˆ(MVP)** ã§ã™ã€‚
æœ¬ç•ªå‘ã‘ã«ã¯ /modules ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸åˆ†å‰²æ¨å¥¨ã€‚

å¿…è¦ç’°å¢ƒå¤‰æ•°:
- OPENAI_API_KEY or AZURE_OPENAI_API_KEYï¼ˆä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦ï¼‰
- SERPAPI_KEYï¼ˆä»»æ„: Googleæ¤œç´¢ä»¶æ•°ç”¨ã€‚æœªè¨­å®šãªã‚‰DuckDuckGoã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

å®Ÿè£…ãƒ¡ãƒ¢:
- Qiitaã¯å…¬å¼APIã‚’å©ãã¾ã™ï¼ˆRate Limit: 60req/minï¼‰ã€‚
- Zenn/Note ã¯å…¬å¼APIãŒå¼±ã„ãŸã‚HTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°(robots.txtç¢ºèªè¦)ã€‚å¿…è¦ã«å¿œã˜ã¦æ‰‹å‹•CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®UIã‚‚ç”¨æ„ã€‚
- Google Trends ã¯ pytrends ã‚’ä½¿ç”¨(ä»»æ„)ã€‚
"""

import os
import json
import time
import re
from typing import List, Dict, Any, Optional, Tuple

import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# -----------------------------
# ----- Auth helper -----------
# -----------------------------


# èªè¨¼æƒ…å ±ã‚’ secrets.toml ã‹ã‚‰å–å¾—ï¼ˆç„¡ã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
AUTH_USERNAME = None
AUTH_PASSWORD = None
try:
    AUTH_USERNAME = st.secrets.get("AUTH_USERNAME")  # type: ignore[attr-defined]
    AUTH_PASSWORD = st.secrets.get("AUTH_PASSWORD")  # type: ignore[attr-defined]
except Exception:
    pass

if not AUTH_USERNAME:
    AUTH_USERNAME = "mellon"
if not AUTH_PASSWORD:
    AUTH_PASSWORD = "pw"

# ã‚·ãƒ³ãƒ—ãƒ«ã« 1 ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ³å®š


def login_screen() -> bool:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã‚’è¡¨ç¤ºã—ã€èªè¨¼æ¸ˆãªã‚‰ True ã‚’è¿”ã™"""
    if st.session_state.get("authenticated"):
        return True

    st.title("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³")
    st.write("ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    user = st.text_input("Username")
    pw = st.text_input("Password", type="password")
    if st.button("Login"):
        if user == AUTH_USERNAME and (AUTH_PASSWORD == "*" or pw == AUTH_PASSWORD):
            st.session_state["authenticated"] = True
            st.success("Login success!")
            # rerun in a version-agnostic way
            if hasattr(st, "experimental_rerun"):
                st.experimental_rerun()
            elif hasattr(st, "rerun"):
                st.rerun()
        else:
            st.error("Invalid credentials")
    st.stop()

# -----------------------------
# ---------- Config -----------
# -----------------------------
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
TEMPERATURE = 0.2
# ----- APIã‚­ãƒ¼è¨­å®š -----
# Streamlit Cloud ç­‰ã§ã¯ st.secretsã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹
# secrets.toml ãŒå­˜åœ¨ã—ãªã„ç’°å¢ƒã§ã¯ st.secrets ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è‡ªä½“ãŒä¾‹å¤–ã«ãªã‚‹ãŸã‚ try ã§åŒ…ã‚€
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    try:
        OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]  # type: ignore[index]
    except Exception:
        OPENAI_API_KEY = ""

# Google Custom Search JSON API ã‚­ãƒ¼ / CX
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CX = os.getenv("GOOGLE_CX")
if not GOOGLE_API_KEY or not GOOGLE_CX:
    try:
        GOOGLE_API_KEY = GOOGLE_API_KEY or st.secrets["GOOGLE_API_KEY"]  # type: ignore[index]
        GOOGLE_CX = GOOGLE_CX or st.secrets["GOOGLE_CX"]  # type: ignore[index]
    except Exception:
        pass
# APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è­¦å‘Šã‚’è¡¨ç¤ºã—ã¦ä»¥é™ã®å‡¦ç†ãŒè½ã¡ãªã„ã‚ˆã†ã«ã™ã‚‹
if not OPENAI_API_KEY:
    st.sidebar.error(
        "OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
        "ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `export OPENAI_API_KEY=YOUR_KEY` ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€"
        ".streamlit/secrets.toml ã« OPENAI_API_KEY ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
    )
else:
    # langchain_openai ã¯ç’°å¢ƒå¤‰æ•°ã‚’å‚ç…§ã™ã‚‹ãŸã‚ä¿è¨¼ã—ã¦ãŠã
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

st.set_page_config(page_title="TS-AI Keyword Research", layout="wide")

# -----------------------------
# ----- Data Models -----------
# -----------------------------
class GeneratedKeyword(BaseModel):
    keyword: str = Field(..., description="æ¤œç´¢ã«ç”¨ã„ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    intent: str = Field(..., description="ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢æ„å›³ã®èª¬æ˜")
    variants: List[str] = Field(default_factory=list, description="é¡ä¼¼/æ´¾ç”Ÿã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

class KeywordSet(BaseModel):
    theme: str
    industry: str
    tech_terms: List[str]
    base_keywords: List[GeneratedKeyword]

# PatternInsight: suggestions ãŒ dict ã§è¿”ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ Any ã§è¨±å®¹
class PatternInsight(BaseModel):
    title_patterns: List[str]
    hashtag_patterns: List[str]
    gaps: List[str]
    suggestions: List[Any]

# -----------------------------
# ----- LLM Helpers -----------
# -----------------------------
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã«ã‚‚å¯¾å¿œã™ã‚‹ãŸã‚ api_key ã‚’å¼•æ•°ã«è¿½åŠ 
@st.cache_resource(show_spinner=False)
def get_llm(model: str = DEFAULT_MODEL, api_key: str = "") -> ChatOpenAI:
    # api_key ãŒæ¸¡ã•ã‚Œã¦ã„ã‚Œã°å„ªå…ˆã€‚ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ãƒ»st.secrets ã‚’ä½¿ç”¨
    key = api_key or os.getenv("OPENAI_API_KEY")
    return ChatOpenAI(model=model, temperature=TEMPERATURE, openai_api_key=key)


def build_keyword_prompt(theme: str, industries: List[str], tech_terms: List[str]) -> ChatPromptTemplate:
    template = (
        """ã‚ãªãŸã¯SEO/ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®è¦ä»¶ã§æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œã‚’ä½œã£ã¦ãã ã•ã„ã€‚\n\n"
        "ä¸»é¡Œ: {theme}\n"
        "æ¥­ç•Œ: {industries}\n"
        "é–¢é€£æŠ€è¡“: {tech_terms}\n\n"
        "å‡ºåŠ›è¦ä»¶:\n"
        "- å„æ¥­ç•ŒÃ—æŠ€è¡“Ã—ä¸»é¡Œã®æ›ã‘åˆã‚ã›ã‚’æ„è­˜ã—ã¦å¤šæ§˜ã«\n"
        "- æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¸­å¿ƒã«ã€å¿…è¦ãªã‚‰è‹±èªã‚‚\n"
        "- 1è¡Œã«1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚intent(æ¤œç´¢æ„å›³)ã¨variants(æ´¾ç”Ÿå½¢)ã‚‚ä»˜ã‘ã‚‹\n"
        "- JSONã§è¿”ã™ã€‚" 
        """
    )
    return ChatPromptTemplate.from_template(template)


keyword_parser = PydanticOutputParser(pydantic_object=List[GeneratedKeyword])


def generate_keywords(llm: ChatOpenAI, theme: str, industries: List[str], tech_terms: List[str]) -> List[GeneratedKeyword]:
    prompt = build_keyword_prompt(theme, industries, tech_terms)
    messages = prompt.format_messages(theme=theme, industries=industries, tech_terms=tech_terms)
    # Force JSON mode via tool like structured output or by asking
    resp = llm.invoke(messages + [
        {"role": "system", "content": "å¿…ãšæœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã€‚ä½™è¨ˆãªæ–‡ç« ç¦æ­¢ã€‚"}
    ])
    text = resp.content.strip()
    try:
        data = json.loads(text)
        return [GeneratedKeyword(**d) for d in data]
    except Exception:
        # fallback: try to extract JSON
        m = re.search(r"\[.*\]", text, re.S)
        if m:
            data = json.loads(m.group(0))
            return [GeneratedKeyword(**d) for d in data]
        st.error("LLMã®å‡ºåŠ›JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚\n\nå‡ºåŠ›:\n" + text)
        return []


pattern_parser = PydanticOutputParser(pydantic_object=PatternInsight)

def analyze_patterns(llm: ChatOpenAI, titles: List[str], tags: List[str], counts: Dict[str, int]) -> PatternInsight:
    template = (
        """ã‚ãªãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ã®ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€Noteã§ã®å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n\n"
        "ã€ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§ã€‘\n{titles}\n\n"
        "ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ä¸€è¦§ã€‘\n{tags}\n\n"
        "ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥è¨˜äº‹æ•°ã€‘\n{counts}\n\n"
        "å‡ºåŠ›è¦ä»¶:\n"
        "- åŠ¹æœçš„ãªã‚¿ã‚¤ãƒˆãƒ«ã®å‹ï¼ˆtitle_patternsï¼‰\n"
        "- å¼·ã„ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆhashtag_patternsï¼‰\n"
        "- è¨˜äº‹ãŒå°‘ãªã„ãŒéœ€è¦ãŒã‚ã‚Šãã†ãªã‚®ãƒ£ãƒƒãƒ—ãƒ†ãƒ¼ãƒï¼ˆgapsï¼‰\n"
        "- å…·ä½“çš„ãªè¨˜äº‹æ¡ˆã®ææ¡ˆï¼ˆsuggestionsï¼‰\n"
        "JSONã§è¿”ã™ã€‚"""
    )
    prompt = ChatPromptTemplate.from_template(template)
    messages = prompt.format_messages(titles=titles, tags=tags, counts=counts)
    resp = llm.invoke(messages + [
        {"role": "system", "content": "å¿…ãšæœ‰åŠ¹ãªJSONã®ã¿ã€‚ä½™è¨ˆãªæ–‡ç« ã¯ç¦æ­¢ã€‚"}
    ])
    text = resp.content.strip()
    try:
        data = json.loads(text)
        return PatternInsight(**data)
    except Exception:
        m = re.search(r"\{.*\}", text, re.S)
        if m:
            data = json.loads(m.group(0))
            return PatternInsight(**data)
        st.error("JSONè§£æå¤±æ•—: " + text)
        raise

# -----------------------------
# ----- Scrapers --------------
# -----------------------------

# Minimalistic wrappers. In production, handle retries, headers, error codes, robots.txt.

USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"


def google_result_count(
    query: str,
    serpapi_key: Optional[str] = None,
    google_api_key: Optional[str] = None,
    google_cx: Optional[str] = None,
) -> int:
    """æ¤œç´¢çµæœä»¶æ•°ã‚’è¿”ã™ã€‚

    å„ªå…ˆé †:
    1. SerpAPI (å®‰å®š / é«˜é€Ÿ)
    2. Google Custom Search JSON API
    3. HTML ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (ç°¡æ˜“)
    """
    if serpapi_key:
        url = "https://serpapi.com/search.json"
        params = {"q": query, "engine": "google", "api_key": serpapi_key}
        r = requests.get(url, params=params, timeout=30)
        r.raise_for_status()
        data = r.json()
        return int(data.get("search_information", {}).get("total_results", 0))

    if google_api_key and google_cx:
        url = "https://customsearch.googleapis.com/customsearch/v1"
        params = {"key": google_api_key, "cx": google_cx, "q": query}
        r = requests.get(url, params=params, timeout=30)
        if r.status_code == 429:  # quota exceeded
            return -2
        r.raise_for_status()
        data = r.json()
        return int(data.get("searchInformation", {}).get("totalResults", 0))

    # fallback: scrape (rough)
    headers = {"User-Agent": USER_AGENT}
    r = requests.get("https://www.google.com/search", params={"q": query}, headers=headers, timeout=30)
    if r.status_code != 200:
        return -1
    m = re.search(r"ç´„?([0-9,]+)ä»¶", r.text)
    if m:
        return int(m.group(1).replace(",", ""))
    return -1


def qiita_search_count(query: str) -> Tuple[int, List[Dict[str, Any]]]:
    url = "https://qiita.com/api/v2/items"
    params = {"query": query, "per_page": 20, "page": 1}
    r = requests.get(url, params=params, timeout=30)
    if r.status_code != 200:
        return -1, []
    items = r.json()
    return len(items), items

# -----------------------------
# ----- Streamlit UI ----------
# -----------------------------

def sidebar_inputs() -> Dict[str, Any]:
    st.sidebar.header("åŸºæœ¬è¨­å®š")
    theme = st.sidebar.text_input("ä¸»é¡Œ", value="æ™‚ç³»åˆ—åˆ†æ")
    inds = st.sidebar.text_input("å¯¾è±¡æ¥­ç•Œ(ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", value="è£½é€ æ¥­,å°å£²æ¥­").split(",")
    inds = [x.strip() for x in inds if x.strip()]
    tech = st.sidebar.text_input("æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", value="åœ¨åº«æœ€é©åŒ–,ç™ºæ³¨è‡ªå‹•åŒ–").split(",")
    tech = [x.strip() for x in tech if x.strip()]

    st.sidebar.header("APIã‚­ãƒ¼è¨­å®š")
    model = st.sidebar.text_input("LLMãƒ¢ãƒ‡ãƒ«", value=DEFAULT_MODEL)
    openai_key = st.sidebar.text_input("OPENAI_API_KEY", type="password", placeholder="sk-...")
    serpapi_key = st.sidebar.text_input("SERPAPI_KEY (ä»»æ„)", type="password")
    google_api_key = st.sidebar.text_input("GOOGLE_API_KEY (ä»»æ„)", type="password")
    google_cx = st.sidebar.text_input("GOOGLE_CX æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ID (ä»»æ„)")

    return {
        "theme": theme,
        "industries": inds,
        "tech": tech,
        "model": model,
        "openai_api_key": openai_key,
        "serpapi_key": serpapi_key,
        "google_api_key": google_api_key,
        "google_cx": google_cx,
    }


def main():
    login_screen() # ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã‚’è¡¨ç¤º

    cfg = sidebar_inputs()
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒAPIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ãŸå ´åˆã¯ç’°å¢ƒå¤‰æ•°ã«ã‚‚è¨­å®šã—ã¦ãŠãï¼ˆå†èª­è¾¼ã‚„ä»–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå‚ç…§ã™ã‚‹å¯èƒ½æ€§ã«å‚™ãˆã‚‹ï¼‰
    if cfg.get("openai_api_key"):
        os.environ["OPENAI_API_KEY"] = cfg["openai_api_key"]

    llm = get_llm(cfg["model"], cfg.get("openai_api_key", ""))

    st.title("SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ")

    # 1) LLMã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
    st.header("1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è‡ªå‹•ç”Ÿæˆ")
    if st.button("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆã™ã‚‹", type="primary"):
        with st.spinner("LLMãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­..."):
            kws = generate_keywords(llm, cfg["theme"], cfg["industries"], cfg["tech"])
        if kws:
            df_kw = pd.DataFrame([k.model_dump() for k in kws])
            st.session_state["keywords_df"] = df_kw
            st.success(f"{len(df_kw)}ä»¶ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ")
    if "keywords_df" in st.session_state:
        st.download_button(
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            st.session_state["keywords_df"].to_csv(index=False).encode("utf-8"),
            file_name="keywords.csv",
            mime="text/csv",
        )
        # è¡¨ç¤ºã‚’å¸¸ã«ç¶­æŒ
        st.dataframe(st.session_state["keywords_df"], use_container_width=True)

    # 2) å„åª’ä½“ã®æ¤œç´¢ï¼†åé›†
    st.header("2. åª’ä½“åˆ¥ æ¤œç´¢ãƒ»ä»¶æ•°é›†è¨ˆ")
    if "keywords_df" not in st.session_state:
        st.info("ã¾ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
    else:
        df_kw = st.session_state["keywords_df"]
        selected_kws = st.multiselect("èª¿æŸ»ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ", df_kw["keyword"].tolist(), df_kw["keyword"].tolist()[:10])
        max_titles = st.number_input("å„åª’ä½“ã‹ã‚‰å–å¾—ã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ä¸Šé™", 5, 100, 20)
        if st.button("æ¤œç´¢ã‚’å®Ÿè¡Œ"):
            serp = cfg["serpapi_key"] or None
            g_counts = {}
            qiita_rows = []
            # Note / Zenn ã¯å–å¾—ã—ãªã„

            progress = st.progress(0.0)
            total = len(selected_kws)
            for i, q in enumerate(selected_kws):
                g_counts[q] = google_result_count(
                    q,
                    serpapi_key=serp,
                    google_api_key=cfg.get("google_api_key") or GOOGLE_API_KEY,
                    google_cx=cfg.get("google_cx") or GOOGLE_CX,
                )
                qiita_c, qiita_items = qiita_search_count(q)
                qiita_rows.extend([{**item, "_kw": q} for item in qiita_items])
                # Note / Zenn ã¯å–å¾—ã—ãªã„
                progress.progress((i + 1) / total)
                time.sleep(0.3)

            df_google = pd.DataFrame({"keyword": list(g_counts.keys()), "google_results": list(g_counts.values())})
            df_qiita = pd.DataFrame(qiita_rows)
            # Note / Zenn ã¯ç”Ÿæˆã—ãªã„

            st.session_state.update({
                "df_google": df_google,
                "df_qiita": df_qiita,
                # Note / Zenn ã¯ä¿å­˜ã—ãªã„
            })
            st.success("æ¤œç´¢å®Œäº†ï¼")

    if "df_google" in st.session_state:
        st.subheader("Googleæ¤œç´¢ä»¶æ•°")
        df_google_disp = st.session_state["df_google"].sort_values("google_results", ascending=False)
        st.dataframe(df_google_disp.reset_index(drop=True), use_container_width=True)
    if "df_qiita" in st.session_state:
        st.subheader("Qiitaè¨˜äº‹ä¸€è¦§(ã„ã„ã­é †)")
        df_qiita_raw = st.session_state["df_qiita"].copy()
        if "tags" in df_qiita_raw.columns:
            df_qiita_raw["tags"] = df_qiita_raw["tags"].apply(
                lambda l: ",".join(t["name"] for t in l) if isinstance(l, list) else ""
            )
        else:
            df_qiita_raw["tags"] = ""

        if "likes_count" in df_qiita_raw.columns:
            df_qiita_raw = df_qiita_raw.sort_values("likes_count", ascending=False)
        qiita_display = df_qiita_raw.reset_index(drop=True)
        display_cols = [c for c in ["_kw", "title", "likes_count", "tags", "url"] if c in qiita_display.columns]
        st.dataframe(qiita_display[display_cols].fillna(""), use_container_width=True)
    # Note / Zenn è¡¨ç¤ºã¯è¡Œã‚ãªã„

    # 3) LLMã§å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    st.header("3. LLMã§å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
    if all(k in st.session_state for k in ["df_qiita", "df_google"]):
        if "title" not in st.session_state["df_qiita"].columns or st.session_state["df_qiita"].empty:
            st.warning("Qiita ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ API ãƒ¬ãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            titles = []
        else:
            titles = st.session_state["df_qiita"]["title"].tolist()
        # Qiita tags
        tag_list = []
        if "df_qiita" in st.session_state and not st.session_state["df_qiita"].empty and "tags" in st.session_state["df_qiita"].columns:
            for tags in st.session_state["df_qiita"]["tags"]:
                if isinstance(tags, list):
                    tag_list.extend([t.get("name", "") for t in tags])
        if titles and st.button("å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"):
            with st.spinner("LLMãŒåˆ†æä¸­..."):
                insight = analyze_patterns(llm, titles, tag_list, st.session_state["df_google"].set_index("keyword")["google_results"].to_dict())
            st.json(insight.model_dump())
    else:
        st.info("æ¤œç´¢çµæœï¼ˆQiita ã¨ Googleï¼‰ãŒæƒã£ã¦ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ã€‚")

    if "df_trend" in st.session_state:
        st.subheader("Google Trends äººæ°—åº¦ (0-100)")
        st.dataframe(st.session_state["df_trend"].sort_values("trend_score", ascending=False), use_container_width=True)

    st.markdown("---")
    st.caption("Â© 2025 Keyword Research MVP")


if __name__ == "__main__":
    main()
